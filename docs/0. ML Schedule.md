### **Missing Value Imputation**

Handling missing values is crucial to prevent biased models or loss of information. Common techniques include:
- **Mean/Median/Mode Imputation:** Replaces missing values with statistical measures.
- **Forward/Backward Fill:** Fills missing values using adjacent values (useful in time-series).
- **KNN Imputation:** Estimates missing values based on similar observations.
#### **Example: Mean Imputation using Scikit-Learn**

```python
from sklearn.impute import SimpleImputer
import pandas as pd

# Example dataset with missing values
df = pd.DataFrame({'A': [1, 2, None, 4], 'B': [None, 2, 3, 4]})

# Mean Imputation
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

print(df_imputed)
```
### **Handling Categorical Features**

Categorical variables must be converted into numerical values for machine learning models. There are several encoding techniques:
- **One-Hot Encoding (OHE):** Converts categories into binary vectors (best for nominal data).
- **Label Encoding:** Assigns a unique number to each category (best for ordinal data).
- **Target Encoding:** Replaces categories with the mean of the target variable (used in regression).
#### **Example: One-Hot and Label Encoding**

```python
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import pandas as pd

# Sample data
df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue']})

# One-Hot Encoding
ohe = OneHotEncoder(sparse_output=False)
encoded_ohe = ohe.fit_transform(df[['Color']])
print("One-Hot Encoded:\n", encoded_ohe)

# Label Encoding
le = LabelEncoder()
encoded_le = le.fit_transform(df['Color'])
print("Label Encoded:", encoded_le)
```
## Outlier Detection
Outliers can distort predictions. Common methods:
- **Z-score Method** (values beyond 3 standard deviations)
- **IQR Method** (removing values outside 1.5*IQR range)

```python
import numpy as np
from scipy import stats

# Example data
data = np.array([10, 12, 15, 100, 14, 13, 16])

# Z-score detection
z_scores = np.abs(stats.zscore(data))
outliers = data[z_scores > 3]
print(outliers)
```
## Feature Scaling
Feature scaling ensures all features contribute equally.
- **Min-Max Scaling** (normalizes values between 0 and 1)
- **Standardization** (zero mean, unit variance)

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Sample data
data = [[100], [200], [300]]

# Min-Max Scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```
### **Feature Construction**

Feature construction is the process of creating new meaningful features from existing ones to improve a machine learning modelâ€™s performance. It helps capture complex relationships in the data.

- **Polynomial Features:** Expands features to higher-degree terms (e.g., x,x2,x3x, x^2, x^3).
- **Interaction Features:** Captures relationships between two or more features by multiplying them.
#### **Example: Polynomial Feature Expansion**

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# Sample dataset with two features
X = np.array([[2, 3]])

# Generate polynomial features up to degree 2
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

print(X_poly)
```

This expands `[2, 3]` into `[1, 2, 3, 4, 6, 9]`, including squared and interaction terms. 
## Feature Selection
Feature selection removes irrelevant variables.
- **Variance Threshold** (drops low-variance features)
- **Recursive Feature Elimination (RFE)**

```python
from sklearn.feature_selection import VarianceThreshold
import numpy as np

X = np.array([[1, 0, 0.1], [0, 1, 0.1], [1, 1, 0.1]])
selector = VarianceThreshold(threshold=0.05)
X_selected = selector.fit_transform(X)
print(X_selected)
```
#### **RFE Implementation in Python using Scikit-Learn**

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Initialize model
model = LogisticRegression()

# Apply RFE (Selecting top 5 features)
rfe = RFE(estimator=model, n_features_to_select=5)
X_selected = rfe.fit_transform(X, y)

# Get selected features
selected_features = rfe.support_
ranking = rfe.ranking_

# Print results
print("Selected Features:", selected_features)
print("Feature Ranking:", ranking)
```

### **Principal Component Analysis (PCA)**

PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining maximum variance. It helps reduce overfitting and speeds up computations in machine learning.

```python
from sklearn.decomposition import PCA
import numpy as np

# Sample dataset (3 samples, 2 features)
X = np.array([[1, 2], [3, 4], [5, 6]])

# Apply PCA to reduce to 1 principal component
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X)

print(X_pca)
```
## Singular Value Decomposition (SVD)
SVD is used for dimensionality reduction in large datasets.

```python
from sklearn.decomposition import TruncatedSVD
import numpy as np

X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
svd = TruncatedSVD(n_components=2)
X_svd = svd.fit_transform(X)
print(X_svd)
