**Status:**  #Complete 
**Tags:**  [[Data Preprocessing]]  [[Feature Engineering]] [[Feature Scaling]]
#### **Normalization**

Normalization is a data preparation technique often used in machine learning to standardize numeric columns to a common scale. It ensures that features contribute equally to the model, without distorting their differences.

---

#### **Types of Normalization**

1. **Min-Max Scaling**
2. **Mean Scaling**
3. **Max Absolute Scaling**
4. **Robust Scaling**

---

#### **1. Min-Max Scaling**

##### **Formula**:

$$X_i' = \frac{X_i - X_{min}}{X_{max} - X_{min}}$$

##### **Purpose**:

- Scales data to a range of [0,1][0, 1].
- Useful when features have different ranges.

---

#### **Implementation**

1. **Load Dataset**:
    
    ```python
    
    df = pd.read_csv('wine_data.csv', header=None, usecols=[0, 1, 2])
    df.columns = ['Class label', 'Alcohol', 'Malic acid']
    df
```
![[Pasted image 20250125131453.png]]
1. **Visualize Original Distributions**:
    
    ```python
    sns.kdeplot(df['Alcohol'])
    ```

![[Pasted image 20250125131627.png]]

```python
    sns.kdeplot(df['Malic acid'])
    ```

![[Pasted image 20250125131602.png]]

1. **Scatter Plot by Class Label**:
    
    ```python
    color_dict = {1: 'red', 3: 'green', 2: 'blue'}
    sns.scatterplot(df['Alcohol'], df['Malic acid'], hue=df['Class label'], palette=color_dict)
    ```

![[Pasted image 20250125131709.png]]


4. **Split Data**:
    
    ```python
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        df.drop('Class label', axis=1),
        df['Class label'],
        test_size=0.3,
        random_state=0
    )
    ```
    Output : ((124, 2), (54, 2))
5. **Apply Min-Max Scaling**:
    
    ```python
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    # fit the scaler to the train set, it will learn the parameters
    scaler.fit(X_train)
    # transform train and test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

    X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)
    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)
np.round(X_train.describe(), 1)
```
![[Pasted image 20250125132024.png]]
```python
np.round(X_train_scaled.describe(), 1)
    ```
![[Pasted image 20250125132056.png]]

6. **Visualizations**:
    
    - **Scatter Plots Before & After Scaling**:
        
        ```python
        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))
        ax1.scatter(X_train['Alcohol'], X_train['Malic acid'], c=y_train)
        ax1.set_title("Before Scaling")
        ax2.scatter(X_train_scaled['Alcohol'], X_train_scaled['Malic acid'], c=y_train)
        ax2.set_title("After Scaling")
        plt.show()
        ```
        ![[Pasted image 20250125132233.png]]
    - **KDE Plots**:
        
        ```python
        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))
        ax1.set_title('Before Scaling')
        sns.kdeplot(X_train['Alcohol'], ax=ax1)
        sns.kdeplot(X_train['Malic acid'], ax=ax1)
        ax2.set_title('After Min-Max Scaling')
        sns.kdeplot(X_train_scaled['Alcohol'], ax=ax2)
        sns.kdeplot(X_train_scaled['Malic acid'], ax=ax2)
        plt.show()
        ```
        ![[Pasted image 20250125132351.png]]
        
    
        
7. **Feature-Specific Analysis**:
    
    - **Alcohol Distribution**:
        
        ```python
        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))
        ax1.set_title('Alcohol Distribution Before Scaling')
        sns.kdeplot(X_train['Alcohol'], ax=ax1)
        ax2.set_title('Alcohol Distribution After Scaling')
        sns.kdeplot(X_train_scaled['Alcohol'], ax=ax2)
        plt.show()
        ```
        ![[Pasted image 20250125132430.png]]
        
        
    - **Malic Acid Distribution**:
        
        ```python
        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))
        ax1.set_title('Malic Acid Distribution Before Scaling')
        sns.kdeplot(X_train['Malic acid'], ax=ax1)
        ax2.set_title('Malic Acid Distribution After Scaling')
        sns.kdeplot(X_train_scaled['Malic acid'], ax=ax2)
        plt.show()
        ```
        ![[Pasted image 20250125132444.png]]
        

---
#### **2. Mean Normalization**

- **Definition**: Centers the data around zero by subtracting the mean and dividing by the range.
- **Purpose**: Scales the data to the range [−1,1][-1, 1], making the mean of the data 0.
- **Formula**: 
$$
X_i' = \frac{X_i - \text{mean}(X)}{\text{max}(X) - \text{min}(X)}
$$


---

#### **3. Max Absolute Scaling**

- **Definition**: Scales data by dividing by the maximum absolute value of the feature, preserving sparsity in the data.
- **Purpose**: Ensures all feature values are within the range [−1,1][-1, 1].
- **Formula**: 
$$X_i' = \frac{X_i}{\text{max}(|X|)}$$


---

#### **4. Robust Scaling**

- **Definition**: Uses the interquartile range (IQR) for scaling, making it robust to outliers.
- **Purpose**: Scales features to reduce the influence of outliers, centering the data around the median.
- **Formula**: 
$$X_i' = \frac{X_i - \text{median}(X)}{\text{IQR}(X)} $$$$where \text{ IQR}(X) = Q3 - Q1.$$

---

### Normalization vs. Standardization

| **Aspect**                | **Normalization**                                                                                        | **Standardization**                                                                                                      |
| ------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Definition**            | Scales data to a specific range (e.g., [0, 1], [-1, 1]).                                                 | Centers the data to have a mean of 0 and a standard deviation of 1.                                                      |
| **Purpose**               | Useful when features have varying scales and a bounded range is required.                                | Useful for algorithms that assume normally distributed features or are sensitive to feature magnitudes (e.g., PCA, SVM). |
|                           |                                                                                                          |                                                                                                                          |
| **Sensitive to Outliers** | Yes, normalization can be affected by outliers due to dependency on min and max values.                  | Less sensitive to outliers but can still be impacted if outliers significantly influence the mean or standard deviation. |
| **Applications**          | - Min-Max Scaling, Max Absolute Scaling.- Suitable for non-parametric models like k-NN, neural networks. | - Z-score Scaling.- Suitable for algorithms like PCA, linear regression, logistic regression, and SVM.                   |
| **Key Point**             | Preserves the range and relative differences between values.                                             | Assumes the data distribution is Gaussian and centers data around zero with unit variance.                               |

---
