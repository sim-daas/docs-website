**Status:**  #Complete 
**Tags:**  [[EDA]]  [[Data Preprocessing]]  [[Feature Engineering]] 

---
### **Principal Component Analysis (PCA): Theory and Mathematics**

Principal Component Analysis (PCA) is a **dimensionality reduction technique** widely used in data analysis, machine learning, and statistics. It transforms high-dimensional data into a lower-dimensional space while preserving as much variance (information) as possible. PCA is particularly useful for visualizing data, reducing noise, and improving computational efficiency.

---

## **1. Intuition Behind PCA**
PCA aims to find the **directions (principal components)** in the data that maximize variance. These directions are orthogonal (uncorrelated) and form a new coordinate system. The first principal component captures the most variance, the second captures the next most, and so on.

- **Key Idea**: Project high-dimensional data onto a lower-dimensional subspace while retaining the most important information.
- **Applications**: Data compression, noise reduction, feature extraction, and visualization.

---

## **2. Mathematical Foundations**

### **2.1. Data Representation**
Let the dataset be represented as a matrix \( X \) of size \( n \times p \), where:
- \( n \): Number of observations (samples).
- \( p \): Number of features (variables).

Each row of \( X \) represents an observation, and each column represents a feature.

### **2.2. Centering the Data**
PCA requires the data to be **centered** (mean subtracted) to ensure the first principal component aligns with the direction of maximum variance.

- Compute the mean of each feature:
  $$
  \mu_j = \frac{1}{n} \sum_{i=1}^n X_{ij}, \quad \text{for } j = 1, 2, \dots, p
  $$
- Subtract the mean from each feature:
  $$
  X_{ij} \leftarrow X_{ij} - \mu_j
  $$

The centered data matrix is denoted as $$ \tilde{X} $$

---

### **2.3. Covariance Matrix**
The covariance matrix \( C \) captures the relationships between features. It is a \( p \times p \) symmetric matrix defined as:
$$C = \frac{1}{n-1} \tilde{X}^T \tilde{X}$$

 $$C_{ij} : Covariance \ between \ feature \ i \ and \ j $$
 $$Diagonal \ elements \ C_{ii} : Variance \ of \ feature  i $$
---

### **2.4. Eigenvalue Decomposition**
PCA relies on the **eigenvalue decomposition** of the covariance matrix \( C \). The goal is to find the eigenvectors and eigenvalues of \( C \).

- **Eigenvectors**: Represent the directions (principal components) in the feature space.
- **Eigenvalues**: Represent the amount of variance captured by each principal component.

The eigenvalue decomposition of \( C \) is:
$$C = V \Lambda V^T$$
 $$V : Matrix \ of \ eigenvectors \ (each \ column \ is  \ an \ eigenvector)$$
$$ \Lambda : Diagonal \ matrix \ of \ eigenvalues$$

---

### **2.5. Selecting Principal Components**
The eigenvectors (principal components) are sorted in descending order of their corresponding eigenvalues. The first \( k \) eigenvectors (with the largest eigenvalues) are selected to form a new basis for the data.

#### Variance Explained
The proportion of total variance explained by the \( i \)-th principal component is:

$$
\text{Variance Explained}_i = \frac{\lambda_i}{\sum_{j=1}^p \lambda_j}
$$

$$where \ \lambda_i \ is \ the \  i-th \ eigenvalue. $$

#### Cumulative Variance
The cumulative variance explained by the first \( k \) components is:

$$
\text{Cumulative Variance} = \sum_{i=1}^k \frac{\lambda_i}{\sum_{j=1}^p \lambda_j}
$$

---

### **2.6. Projection onto Principal Components**
The centered data \( \tilde{X} \) is projected onto the selected \( k \) principal components to obtain the lower-dimensional representation \( Z \):
$$Z = \tilde{X} V_k$$

$$ V_k : Matrix\ containing\ the\ first\ k \ eigenvectors \ size \ p \times k$$
$$ Z : Transformed \ data\ in\ the\ lower-dimensional\ space\ size  n \times k $$

---

## **3. Steps of PCA**

1. **Standardize the Data**: Center the data by subtracting the mean of each feature.
2. **Compute the Covariance Matrix**: Calculate the covariance matrix \( C \).
3. **Perform Eigenvalue Decomposition**: Find the eigenvectors and eigenvalues of \( C \).
4. **Sort Eigenvectors**: Sort the eigenvectors in descending order of their eigenvalues.
5. **Select Principal Components**: Choose the top \( k \) eigenvectors that explain the desired amount of variance.
6. **Project the Data**: Transform the data into the new subspace using the selected eigenvectors.

---

## **4. Mathematics of PCA in Detail**

### **4.1. Maximizing Variance**
PCA seeks to find a direction \( w \) (a unit vector) such that the variance of the projected data is maximized:
# Principal Component Analysis (PCA)

## Optimization Problem

PCA seeks to find a direction \( w \) (a unit vector) such that the variance of the projected data is maximized:

$$
\text{Var}(Z) = \text{Var}(w^T \tilde{X}) = w^T C w
$$

The optimization problem is:

$$
\max_w w^T C w \quad \text{subject to} \quad \|w\| = 1
$$

Using Lagrange multipliers, the solution is the eigenvector of \( C \) corresponding to the largest eigenvalue.

---

### **4.2. Singular Value Decomposition (SVD)**
PCA can also be performed using **Singular Value Decomposition (SVD)**. The centered data matrix 
The centered data matrix \( \tilde{X} \) is decomposed as:

$$
\tilde{X} = U \Sigma V^T
$$

where:
- \( U \) : Left singular vectors (size \( n \times n \)).
- \( \Sigma \) : Diagonal matrix of singular values (size \( n \times p \)).
- \( V \) : Right singular vectors (size \( p \times p \)).

The principal components are the columns of \( V \), and the singular values \( \Sigma \) are related to the eigenvalues of the covariance matrix \( C \):

$$
C = \frac{1}{n-1} \tilde{X}^T \tilde{X} = V \left( \frac{\Sigma^2}{n-1} \right) V^T
$$


---

## **5. Choosing the Number of Components (\( k \))**
The number of principal components \( k \) is chosen based on:
1. **Scree Plot**: A plot of eigenvalues in descending order. The "elbow" point indicates the optimal \( k \).
2. **Cumulative Variance**: Select \( k \) such that the cumulative variance explained exceeds a threshold (e.g., 95%).
3. **Kaiser Criterion**: Retain components with eigenvalues greater than 1 (for standardized data).

---

## **6. Advantages of PCA**
- **Dimensionality Reduction**: Reduces the number of features while preserving variance.
- **Noise Reduction**: Removes less important components, which often correspond to noise.
- **Visualization**: Enables 2D or 3D visualization of high-dimensional data.
- **Uncorrelated Features**: Principal components are orthogonal, eliminating multicollinearity.

---

## **7. Limitations of PCA**
- **Linear Assumption**: PCA assumes linear relationships between features.
- **Interpretability**: Principal components are linear combinations of original features and may lack interpretability.
- **Sensitive to Scaling**: PCA is affected by the scale of features, so standardization is often required.

---

## **8. Example**


Given a dataset \( X \) with 3 features, PCA is performed as follows:

1. **Center the data**:  
   $$   \tilde{X} = X - \mu   $$
   where $$ \mu \ is\ the\ mean\ of\ each\ feature. $$ 

2. **Compute the covariance matrix**:  
   $$
   C = \frac{1}{n-1} \tilde{X}^T \tilde{X}
   $$

3. **Perform eigenvalue decomposition**:  
   $$
   C = V \Lambda V^T
   $$
   $$where\ V\  contains\ eigenvectors\ and\ \Lambda\ is\ the\ diagonal\ matrix\ of\ eigenvalues$$

4. **Select the top 2 eigenvectors**:  
    
   $$ Extract\ the\ two\ eigenvectors\ corresponding\ to\ the\ largest\ eigenvalues\ :\    V_2   $$

5. **Project the data onto the new subspace**:  
   $$   Z = \tilde{X} V_2$$
   

The transformed data \( Z \) is now in 2D, making it suitable for visualization or further analysis.

