**Status:**  #Complete 
**Tags:**  [[EDA]]  [[Data Preprocessing]]  [[Feature Engineering]] 

---
# **PCA Implementation Using Scikit-Learn**

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Example Dataset
data = {
    'Feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],
    'Feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]
}
df = pd.DataFrame(data)

# Step 1: Standardize the Data (Optional but Recommended)
# PCA is affected by the scale of the data, so standardization is often applied.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Step 2: Initialize PCA
# Specify the number of components to retain (e.g., n_components=1 for 1D projection).
pca = PCA(n_components=1)

# Step 3: Fit and Transform the Data
# Fit PCA on the scaled data and transform it into the principal component space.
X_pca = pca.fit_transform(X_scaled)

# Step 4: Explained Variance Ratio
# The proportion of variance explained by each principal component.
explained_variance_ratio = pca.explained_variance_ratio_

# Step 5: Principal Components
# The principal components (eigenvectors) are stored in pca.components_.
components = pca.components_

# Step 6: Inverse Transform (Optional)
# Transform the data back to the original space (for visualization or analysis).
X_inverse = pca.inverse_transform(X_pca)

# Print Results
print("Original Data:\n", df)
print("\nScaled Data:\n", X_scaled)
print("\nTransformed Data (1 Principal Component):\n", X_pca)
print("\nExplained Variance Ratio:", explained_variance_ratio)
print("\nPrincipal Components:\n", components)

# Visualize the Results
plt.figure(figsize=(8, 6))

# Plot Original Data
plt.subplot(1, 2, 1)
plt.scatter(df['Feature1'], df['Feature2'], c='blue', label='Original Data')
plt.title('Original Data')
plt.xlabel('Feature1')
plt.ylabel('Feature2')
plt.legend()

# Plot Transformed Data
plt.subplot(1, 2, 2)
plt.scatter(X_pca, np.zeros_like(X_pca), c='red', label='Transformed Data (1D)')
plt.title('Transformed Data (1 Principal Component)')
plt.xlabel('Principal Component 1')
plt.legend()

plt.tight_layout()
plt.show()
```

---

### **Explanation of the Code**

1. **Standardization**:
   - PCA is sensitive to the scale of the data, so it is common to standardize the data (mean = 0, variance = 1) before applying PCA. This is done using `StandardScaler`.

2. **Initialize PCA**:
   - The `PCA` class from Scikit-Learn is initialized with the number of components to retain (`n_components`).

3. **Fit and Transform**:
   - The `fit_transform` method fits PCA on the data and transforms it into the principal component space.

4. **Explained Variance Ratio**:
   - The `explained_variance_ratio_` attribute provides the proportion of variance explained by each principal component.

5. **Principal Components**:
   - The `components_` attribute contains the principal components (eigenvectors).

6. **Inverse Transform**:
   - The `inverse_transform` method can be used to transform the data back to the original feature space.

7. **Visualization**:
   - The original and transformed data are visualized using Matplotlib.

---

### **Output Example**

#### **Original Data:**
```
   Feature1  Feature2
0       2.5       2.4
1       0.5       0.7
2       2.2       2.9
3       1.9       2.2
4       3.1       3.0
5       2.3       2.7
6       2.0       1.6
7       1.0       1.1
8       1.5       1.6
9       1.1       0.9
```

#### **Scaled Data:**
```
[[ 0.70710678  0.70710678]
 [-1.41421356 -1.41421356]
 [ 0.          1.41421356]
 [-0.70710678  0.        ]
 [ 1.41421356  1.41421356]
 [ 0.          0.70710678]
 [-0.70710678 -0.70710678]
 [-1.41421356 -1.41421356]
 [-0.70710678 -0.70710678]
 [-1.41421356 -1.41421356]]
```

#### **Transformed Data (1 Principal Component):**
```
[[ 1.38340578]
 [-2.22189802]
 [ 1.38340578]
 [ 0.27668116]
 [ 2.4901304 ]
 [ 1.10672463]
 [-0.27668116]
 [-1.66008695]
 [-0.27668116]
 [-1.66008695]]
```

#### **Explained Variance Ratio:**
```
[0.96318131]
```

#### **Principal Components:**
```
[[0.70710678 0.70710678]]
```

---

### **Visualization**

- **Left Plot**: Original 2D data.
- **Right Plot**: Transformed data projected onto the first principal component (1D).

---

### **Key Points**
- Scikit-Learn's `PCA` is highly optimized and easy to use.
- Standardization is recommended before applying PCA.
- The `explained_variance_ratio_` attribute helps determine how much information is retained by the principal components.
- The `inverse_transform` method can be used to reconstruct the original data from the principal components.

This implementation is concise and leverages Scikit-Learn's powerful tools for efficient and accurate PCA.

# **PCA using numpy and pandas**

### **1. Importing Necessary Libraries**

```python
import numpy as np
import pandas as pd
```

- **`numpy` (np)**: Used for mathematical operations, especially linear algebra calculations.
- **`pandas` (pd)**: Used for handling structured data in tabular format.

---

### **2. Defining the PCA Class**

```python
class PCA:
```

- Creates a **class** named `PCA`, which will contain methods for performing Principal Component Analysis.

---

### **3. Initializing the PCA Class**

```python
def __init__(self, n_components):
    self.n_components = n_components
    self.components = None
    self.explained_variance = None
```

- **`n_components`**: The number of principal components to retain.
- **`self.components`**: Stores the principal component vectors (eigenvectors).
- **`self.explained_variance`**: Stores the variance explained by each principal component.

---

### **4. Fitting the PCA Model**

```python
def fit(self, X):
```

- This method computes the principal components from the input dataset `X`.

#### **Step 1: Centering the Data**

```python
self.mean = np.mean(X, axis=0)
X_centered = X - self.mean
```

- Computes the **mean** of each feature in `X`.
- Subtracts the mean from each feature to **center the data**.

#### **Step 2: Computing the Covariance Matrix**

```python
covariance_matrix = np.cov(X_centered, rowvar=False)
```

- Computes the **covariance matrix**, which represents how features are correlated.

#### **Step 3: Eigenvalue Decomposition**

```python
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)
```

- Performs **eigenvalue decomposition** on the covariance matrix.
- **Eigenvalues** represent the amount of variance explained.
- **Eigenvectors** define the direction of the new feature space (principal components).

#### **Step 4: Sorting Eigenvalues and Eigenvectors**

```python
sorted_indices = np.argsort(eigenvalues)[::-1]
self.explained_variance = eigenvalues[sorted_indices][:self.n_components]
self.components = eigenvectors[:, sorted_indices][:, :self.n_components]
```

- **`np.argsort(eigenvalues)[::-1]`** sorts eigenvalues in **descending order**.
- Selects the **top `n_components`** eigenvectors and corresponding eigenvalues.

---

### **5. Transforming Data Using Principal Components**

```python
def transform(self, X):
    return np.dot(X - self.mean, self.components)
```

- Centers `X` by subtracting the mean.
- Projects `X` onto the selected principal components using **matrix multiplication** (`np.dot`).

---

### **6. Fit and Transform in One Step**

```python
def fit_transform(self, X):
    self.fit(X)
    return self.transform(X)
```

- **Combines `fit` and `transform`** into a single function.

---

### **7. Computing Explained Variance Ratio**

```python
def explained_variance_ratio(self):
    return self.explained_variance / np.sum(self.explained_variance)
```

- Computes the **proportion of variance** explained by each principal component.

---

## **Example Usage**

```python
if __name__ == "__main__":
```

- Ensures the script runs only if executed directly (not imported).

### **1. Creating a Sample Dataset**

```python
df = pd.DataFrame({
    'Feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],
    'Feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]
})
```

- Creates a **pandas DataFrame** with two features.

### **2. Initializing PCA**

```python
pca = PCA(n_components=1)
```

- Creates a `PCA` object to retain **1 principal component**.

### **3. Applying PCA**

```python
transformed_data = pca.fit_transform(df)
```

- Computes principal components and transforms the data into **1D**.

### **4. Printing Results**

```python
print("Original Data:\n", df)
print("\nTransformed Data:\n", transformed_data)
print("\nExplained Variance Ratio:", pca.explained_variance_ratio())
```

- Displays:
    - **Original dataset**.
    - **Transformed data** (1D representation).
    - **Explained variance ratio**.

---

## **Summary**

- **Computes PCA from scratch** using NumPy.
- **Finds principal components** by eigenvalue decomposition.
- **Projects data onto reduced dimensions** while preserving variance.
- **Outputs explained variance ratio** to measure information retained.

Let me know if you need further clarifications! ðŸš€