**Status:**  #Complete 
**Tags:**  [[Data Preprocessing]]  [[Feature Engineering]] [[Encoding]]

---

One Hot Encoding used for nominal encoding

![[Pasted image 20250125141059.png]]

---
### Dummy Variable Trap and Multicollinearity

#### **1. Dummy Variable Trap**

The **dummy variable trap** is a situation in regression analysis when one or more of the dummy variables are highly correlated. This arises when dummy variables are created for each category of a categorical variable, and one of these variables can be perfectly predicted from the others.

**Why it Occurs:**

- When all categories of a variable are encoded, the information for one category becomes redundant, leading to **perfect multicollinearity** (a linear dependency among the variables).

**Example:**

- Consider a categorical variable `Color` with three categories: Red, Green, and Blue.
- If you create dummy variables for all three categories:
    - `Color_Red`, `Color_Green`, and `Color_Blue`
    - The sum of these variables for each data point will always be 1, i.e., `Color_Red + Color_Green + Color_Blue = 1`.

This results in redundancy and introduces multicollinearity.

**Solution:**

- Use **K-1 encoding** (drop one dummy variable). For `Color`, you would create dummies for only two categories, e.g., `Color_Red` and `Color_Green`, and infer the third (`Color_Blue`) when both are 0.

**Code Example:**

```python
pd.get_dummies(df, columns=['Color'], drop_first=True)
```

---

#### **2. Multicollinearity**

**Definition:** Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning that one can be linearly predicted from the others. This makes it difficult for the model to determine the individual effect of each variable.

**Causes:**

1. Dummy Variable Trap: As discussed, including all dummy variables for a categorical feature.
2. Features Derived from Each Other: For example, a variable `Total Sales` and its components `Online Sales` and `Offline Sales`.
3. Strong Correlation Among Variables: E.g., `Height` and `Weight` in a dataset may be highly correlated.

**Why itâ€™s a Problem:**

- Coefficients of the regression model become unstable.
- Reduces the interpretability of the model.
- Can inflate the variance of coefficients, leading to less reliable results.


#### **Example with Multicollinearity**

**Input Data:**

|Car|Engine_Size|Mileage|Engine_Power|Selling_Price|
|---|---|---|---|---|
|Car A|1.2|18|100|450000|
|Car B|1.5|15|120|500000|
|Car C|2.0|10|150|650000|

---

#### 1. OneHotEncoding using Pandas

**Code:**

```python
pd.get_dummies(df, columns=['fuel', 'owner'])
```

**Example Input Data (`df`):**

|brand|km_driven|fuel|owner|selling_price|
|---|---|---|---|---|
|Maruti|145500|Diesel|First Owner|450000|
|Skoda|120000|Diesel|Second Owner|370000|
|Honda|140000|Petrol|Third Owner|158000|
|Hyundai|127000|Diesel|First Owner|225000|
|Maruti|120000|Petrol|First Owner|130000|

**Output Data:**

|brand|km_driven|selling_price|fuel_CNG|fuel_Diesel|fuel_LPG|fuel_Petrol|owner_First Owner|owner_Fourth & Above Owner|owner_Second Owner|owner_Test Drive Car|owner_Third Owner|
|---|---|---|---|---|---|---|---|---|---|---|---|
|Maruti|145500|450000|0|1|0|0|1|0|0|0|0|
|Skoda|120000|370000|0|1|0|0|0|0|1|0|0|
|Honda|140000|158000|0|0|0|1|0|0|0|0|1|
|Hyundai|127000|225000|0|1|0|0|1|0|0|0|0|
|Maruti|120000|130000|0|0|0|1|1|0|0|0|0|

---

#### 2. K-1 OneHotEncoding

**Code:**

```python
pd.get_dummies(df, columns=['fuel', 'owner'], drop_first=True)
```

**Output Data:**

|brand|km_driven|selling_price|fuel_Diesel|fuel_LPG|fuel_Petrol|owner_Fourth & Above Owner|owner_Second Owner|owner_Test Drive Car|owner_Third Owner|
|---|---|---|---|---|---|---|---|---|---|
|Maruti|145500|450000|1|0|0|0|0|0|0|
|Skoda|120000|370000|1|0|0|0|1|0|0|
|Honda|140000|158000|0|0|1|0|0|0|1|
|Hyundai|127000|225000|1|0|0|0|0|0|0|
|Maruti|120000|130000|0|0|1|0|0|0|0|

---

#### 3. One Hot Encoding using Sklearn

**Code:**

```python
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(drop='first', sparse=False, dtype=np.int32)
X_train_new = ohe.fit_transform(X_train[['fuel', 'owner']])
```

**Example Input Data (`X_train`):**

|brand|km_driven|fuel|owner|
|---|---|---|---|
|Hyundai|35000|Diesel|First Owner|
|Jeep|60000|Diesel|First Owner|
|Hyundai|25000|Petrol|First Owner|
|Mahindra|130000|Diesel|Second Owner|
|Hyundai|155000|Diesel|First Owner|

**Transformed Data (`X_train_new`):**

|fuel_Diesel|fuel_LPG|fuel_Petrol|owner_Second Owner|owner_Test Drive Car|owner_Third Owner|
|---|---|---|---|---|---|
|1|0|0|0|0|0|
|1|0|0|0|0|0|
|0|0|1|0|0|0|
|1|0|0|1|0|0|
|1|0|0|0|0|0|

---

#### 4. One Hot Encoding with Top Categories

**Code:**

```python
counts = df['brand'].value_counts()
threshold = 100  # Set threshold for rare categories
repl = counts[counts <= threshold].index
pd.get_dummies(df['brand'].replace(repl, 'uncommon'))
```

**Example Input Data:**

|brand|km_driven|fuel|owner|selling_price|
|---|---|---|---|---|
|Maruti|145500|Diesel|First Owner|450000|
|BMW|120000|Diesel|Second Owner|370000|
|Honda|140000|Petrol|Third Owner|158000|
|Hyundai|127000|Diesel|First Owner|225000|
|Volkswagen|120000|Petrol|First Owner|130000|

**Output Data (with Rare Brands Grouped):**

|BMW|Honda|Hyundai|Maruti|Volkswagen|uncommon|
|---|---|---|---|---|---|
|0|0|0|1|0|0|
|1|0|0|0|0|0|
|0|1|0|0|0|0|
|0|0|1|0|0|0|
|0|0|0|0|1|0|

---
