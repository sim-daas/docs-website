**Status:**  #Complete 
**Tags:**   [[Data Preprocessing]]  [[Feature Engineering]] [[Sklearn]]

---

Pipelines chain together multiple steps so that the output of each step is used as input to the next step.  
Pipelines make it easy to apply the same preprocessing to both train and test datasets!

---
# **Code**

### Step 1: Importing Required Libraries

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.tree import DecisionTreeClassifier
```

### Explanation:

- **`numpy` & `pandas`**: Libraries for data manipulation and numerical operations.
- **`scikit-learn` modules**: For machine learning preprocessing, feature selection, and modeling.

---

### Step 2: Loading the Data

```python
df = pd.read_csv('train.csv')
df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)
```

### Explanation:

- **`pd.read_csv()`**: Loads the Titanic dataset.
- Dropping irrelevant columns (`PassengerId`, `Name`, `Ticket`, and `Cabin`) as they do not contribute to predictions.

---

### Step 3: Splitting the Dataset

```python
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(columns=['Survived']),
    df['Survived'],
    test_size=0.2,
    random_state=42
)
```

### Explanation:

- **`train_test_split()`**: Splits the data into training (80%) and testing (20%) sets.
- `X` contains features, and `y` contains the target (`Survived`).

---

### Step 4: Building Transformers

#### **Imputation Transformer**

```python
trf1 = ColumnTransformer([
    ('impute_age', SimpleImputer(), [2]),
    ('impute_embarked', SimpleImputer(strategy='most_frequent'), [6])
], remainder='passthrough')
```

**Explanation**:

- **`impute_age`**: Replaces missing values in the `Age` column with the mean.
- **`impute_embarked`**: Fills missing values in the `Embarked` column with the most frequent value.

---

#### **One-Hot Encoding**

```python
trf2 = ColumnTransformer([
    ('ohe_sex_embarked', OneHotEncoder(sparse=False, handle_unknown='ignore'), [1, 6])
], remainder='passthrough')
```

**Explanation**:

- Converts categorical columns (`Sex`, `Embarked`) into numerical form using one-hot encoding.

---

#### **Scaling**

```python
trf3 = ColumnTransformer([
    ('scale', MinMaxScaler(), slice(0, 10))
])
```

**Explanation**:

- Scales numerical features to a 0-1 range using `MinMaxScaler`.

---

#### **Feature Selection**

```python
trf4 = SelectKBest(score_func=chi2, k=8)
```

**Explanation**:

- **`SelectKBest`**: Selects the top 8 features based on their chi-squared scores.

---

#### **Model**

```python
trf5 = DecisionTreeClassifier()
```

**Explanation**:

- A **decision tree classifier** is used to predict survival.

---

### Step 5: Building a Pipeline

```python
pipe = Pipeline([
    ('trf1', trf1),
    ('trf2', trf2),
    ('trf3', trf3),
    ('trf4', trf4),
    ('trf5', trf5)
])
```

**Explanation**:

- Combines all transformers and the model into a single pipeline for streamlined preprocessing and training.

---

### Step 6: Training the Pipeline

```python
pipe.fit(X_train, y_train)
```

**Explanation**:

- Trains the pipeline on the training data.

---

### Step 7: Cross-Validation

```python
from sklearn.model_selection import cross_val_score
cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()
```

**Explanation**:

- Performs cross-validation on the pipeline to evaluate its performance.

---

### Step 8: Grid Search

```python
params = {'trf5__max_depth': [1, 2, 3, 4, 5, None]}
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)
```

**Explanation**:

- **`GridSearchCV`**: Tunes hyperparameters (`max_depth` of the decision tree) using cross-validation.
- **`grid.best_score_`**: Shows the best accuracy.
- **`grid.best_params_`**: Displays the best parameters.

---

### Step 9: Exporting the Pipeline

```python
import pickle
pickle.dump(pipe, open('pipe.pkl', 'wb'))
```

**Explanation**:

- Saves the trained pipeline as a `.pkl` file for reuse.

---

## Summary:

1. The pipeline preprocesses the Titanic dataset through imputation, encoding, scaling, feature selection, and classification.
2. **Model**: A decision tree classifier.
3. Hyperparameters are optimized using **grid search**.
4. The final pipeline is saved as a `.pkl` file for future predictions.

---
