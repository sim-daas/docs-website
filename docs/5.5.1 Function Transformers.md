**Status:**  #Complete 
**Tags:**   [[Data Preprocessing]]  [[Feature Engineering]] [[Sklearn]] [[Function Transformer]]
# Import necessary libraries

```python
import pandas as pd
import numpy as np

import scipy.stats as stats

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
```
### Load dataset

```python
# Load dataset
df = pd.read_csv('train.csv', usecols=['Age', 'Fare', 'Survived'])
df.head()
```
![[Pasted image 20250128161000.png]]
```python
# Check for missing values
df.isnull().sum()
```
![[Pasted image 20250128161045.png]]
```python
# Impute missing values in 'Age' with the mean
df['Age'].fillna(df['Age'].mean(), inplace=True)
df.head()
```
![[Pasted image 20250128161116.png]]
### Explanation:
- **Libraries:** Loaded essential libraries for data manipulation, visualization, statistical analysis, and machine learning.
- **Dataset:** Selected only `Age`, `Fare`, and `Survived` columns for simplicity.
- Checked for missing values.
- Imputed missing values in `Age` with the mean to handle nulls.

```python
# Split dataset into features (X) and target (y)
X = df.iloc[:, 1:3]
y = df.iloc[:, 0]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Visualize distributions
plt.figure(figsize=(14, 4))
plt.subplot(121)
sns.histplot(X_train['Age'], kde=True)
plt.title('Age PDF')

plt.subplot(122)
stats.probplot(X_train['Age'], dist="norm", plot=plt)
plt.title('Age QQ Plot')
plt.show()
```
![[Pasted image 20250128161138.png]]
### Explanation:
- Split data into training and testing subsets.
- Visualized `Age` for normality using PDFs and Q-Q plots.

```python
plt.figure(figsize=(14, 4))

# Plot for Fare PDF
plt.subplot(121)
sns.histplot(X_train['Fare'], kde=True)  # Using histplot as distplot is deprecated
plt.title('Fare PDF')

# QQ Plot for Fare
plt.subplot(122)
stats.probplot(X_train['Fare'], dist="norm", plot=plt)
plt.title('Fare QQ Plot')

plt.show()
```
![[Pasted image 20250128161237.png]]
### Explanation:

- **PDF Plot:** Shows the distribution of `Fare` along with the kernel density estimate (KDE) for smooth visualization.
- **Q-Q Plot:** Assesses whether the distribution of `Fare` aligns with a normal distribution.
```python
# Logistic Regression and Decision Tree without transformation
clf = LogisticRegression()
clf2 = DecisionTreeClassifier()

clf.fit(X_train, y_train)
clf2.fit(X_train, y_train)

y_pred = clf.predict(X_test)
y_pred1 = clf2.predict(X_test)

print("Accuracy LR", accuracy_score(y_test, y_pred))
print("Accuracy DT", accuracy_score(y_test, y_pred1))
```

### Explanation:
- Baseline models show initial accuracy without transformations:
  - Logistic Regression: ~65%.
  - Decision Tree: ~69%.

```python
# Logarithmic transformation
trf = FunctionTransformer(func=np.log1p)
X_train_transformed = trf.fit_transform(X_train)
X_test_transformed = trf.transform(X_test)

clf = LogisticRegression()
clf2 = DecisionTreeClassifier()

clf.fit(X_train_transformed, y_train)
clf2.fit(X_train_transformed, y_train)

y_pred = clf.predict(X_test_transformed)
y_pred1 = clf2.predict(X_test_transformed)

print("Accuracy LR", accuracy_score(y_test, y_pred))
print("Accuracy DT", accuracy_score(y_test, y_pred1))
```
### Output
```
Accuracy LR 0.6815642458100558
Accuracy DT 0.6703910614525139
```
### Explanation:
- Applied logarithmic transformation to normalize data.
- Accuracy after transformation:
  - Logistic Regression: Improved to ~68%.
  - Decision Tree: Slightly decreased.

```python
X_transformed2 = trf2.fit_transform(X)

clf = LogisticRegression()
clf2 = DecisionTreeClassifier()

print("LR",np.mean(cross_val_score(clf,X_transformed2,y,scoring='accuracy',cv=10)))
print("DT",np.mean(cross_val_score(clf2,X_transformed2,y,scoring='accuracy',cv=10)))
```
### Output
```
LR 0.6712609238451936
DT 0.6599750312109862
```
### Explanation:
- Applied log transformation only to `Fare`.
- Accuracy was consistent with the full transformation approach.

```python
# Generalized transformation function
def apply_transform(transform):
    X = df.iloc[:, 1:3]
    y = df.iloc[:, 0]

    trf = ColumnTransformer([
        ('log', FunctionTransformer(transform), ['Fare'])
    ], remainder='passthrough')

    X_trans = trf.fit_transform(X)

    clf = LogisticRegression()

    print("Accuracy", np.mean(cross_val_score(clf, X_trans, y, scoring='accuracy', cv=10)))

    plt.figure(figsize=(14, 4))

    plt.subplot(121)
    stats.probplot(X['Fare'], dist="norm", plot=plt)
    plt.title('Fare Before Transform')

    plt.subplot(122)
    stats.probplot(X_trans[:, 0], dist="norm", plot=plt)
    plt.title('Fare After Transform')

    plt.show()

apply_transform(np.sin)
```
![[Pasted image 20250128161725.png]]
### Explanation:
- Created a flexible function to apply any transformation (e.g., sine).
- Accuracy and visual comparison of `Fare` before and after transformation are shown.
- Sine transformation yielded lower accuracy (~62%).
