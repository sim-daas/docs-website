**Status:**  #Complete 
**Tags:**   [[Data Preprocessing]]  [[Feature Engineering ]] [[Function Transformer]]
# Box-Cox Transformer
![[Pasted image 20250128182347.png]]

We use a variable called **lambda (λ)** that can take values between **-5 and 5**. We test all possible values of λ to find the one that makes the data look most like a **normal distribution**. Once we find the best λ, we use it to transform the data for better analysis. This process is part of techniques like the **Box-Cox transformation**.
### Key Points:
1. **Lambda (λ)**: A variable that controls the transformation.
2. **Range of λ**: Test values from -5 to 5.
3. **Goal**: Find the λ that makes the data closest to a normal distribution.
4. **Outcome**: Use the best λ to transform the data.

# Yeo Johnson Transformer
![[asdasd.png]]
The **Yeo-Johnson transformation** is an improved version of the **Box-Cox transformation**. Unlike Box-Cox, which only works with **positive numbers**, Yeo-Johnson can handle **both positive and negative numbers**. This makes it more flexible and widely applicable for transforming data into a normal distribution.
### Key Points:
1. **Improvement**: Yeo-Johnson is an adjustment to Box-Cox.
2. **Handles Negative Numbers**: Works with **all real numbers** (positive, negative, or zero).
3. **Flexibility**: Can be applied to a wider range of datasets.
4. **Goal**: Transforms data to make it more normally distributed.

In short, Yeo-Johnson is a more versatile tool for data transformation!

---
### **1. Importing Libraries**

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats
```

**Explanation:**

- **Numpy** and **Pandas**: Essential for numerical computations and data manipulation.
- **Seaborn** and **Matplotlib**: Used for data visualization and exploratory data analysis (EDA).
- **Scipy.stats**: Provides statistical functions for probability distributions and hypothesis testing.

---

### **2. Additional Imports for Modeling**

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.preprocessing import PowerTransformer
```

**Explanation:**

- **train_test_split**: Splits the dataset into training and testing subsets.
- **cross_val_score**: Performs cross-validation to evaluate model performance.
- **LinearRegression**: Implements a simple linear regression algorithm.
- **r2_score**: Measures the goodness of fit of the regression model.
- **PowerTransformer**: Applies power transformations to make data more Gaussian-like.

---

### **3. Loading the Dataset**

```python
df = pd.read_csv('concrete_data.csv')
```

**Explanation:**

- Loaded the `concrete_data.csv` file into a Pandas DataFrame for analysis. This dataset likely contains features related to concrete mixtures and their corresponding strength values.

---

### **4. Viewing Initial Rows**

```python
df.head()
```
![[Pasted image 20250128162419.png]]
**Explanation:**

- Displayed the first few rows of the dataset to understand its structure, including feature names, sample values, and target variable (`Strength`).

---

### **5. Dataset Shape**

```python
df.shape
```

**Output:** `(1030, 9)`

**Explanation:**

- The dataset contains **1030 rows** and **9 columns**. This helps assess the dataset size and determine the number of samples and features available.

---

### **6. Checking Missing Values**

```python
df.isnull().sum()
```

**Output:** No missing values in the table.

**Explanation:**

- Identified missing values in each column. Since there are none, no further handling is required.

---

### **7. Statistical Summary**

```python
df.describe()
```
![[Pasted image 20250128162515.png]]
**Explanation:**

- Generated key statistics for numerical columns:
    - **Mean, Standard Deviation (std), Min, Max, and Quartiles**.
    - Useful for identifying data distribution and potential outliers.

---

### **8. Splitting Features and Target**

```python
X = df.drop(columns=['Strength'])
y = df['Strength']
```

**Explanation:**

- **X (Features)**: Contains independent variables that influence the target.
- **y (Target)**: Dependent variable (`Strength`), which represents the outcome the model will predict.

---

### **9. Splitting the Dataset**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**Explanation:**

- Divided the dataset into:
    - **Training Set (80%)**: Used to train the model.
    - **Testing Set (20%)**: Used to evaluate the model's performance.
- Set `random_state=42` for reproducibility.

---

### **10. Applying Linear Regression Without Transformation**

```python
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
r2_score(y_test, y_pred)
```

**Output:** `0.6275531792314851`

**Explanation:**

- Trained a **Linear Regression** model on the untransformed data.
- Evaluated the model using the **R2 score**, which indicates that **62.75% of the variance** in the target variable is explained by the model.

---

### **11. Cross-Validation Without Transformation**

```python
np.mean(cross_val_score(lr, X, y, scoring='r2'))
```

**Output:** `0.46099404916628633`

**Explanation:**

- Performed **cross-validation** to assess the model's consistency.
- The lower cross-validated R2 score suggests potential overfitting or inconsistency in performance across different data splits.

---

### **12. Visualizing Distributions Without Transformation**

```python
for col in X_train.columns:
    plt.figure(figsize=(14, 4))
    plt.subplot(121)
    sns.distplot(X_train[col])
    plt.title(col)

    plt.subplot(122)
    stats.probplot(X_train[col], dist="norm", plot=plt)
    plt.title(col)

    plt.show()
```

![[Pasted image 20250128162912.png]]
![[Pasted image 20250128162931.png]]
![[Pasted image 20250128162950.png]]
![[Pasted image 20250128163003.png]]

**Explanation:**

- **Left Plot (PDF)**:
    - Visualizes the probability density of each feature using `sns.distplot()`.
    - Helps identify skewness in the data.
- **Right Plot (QQ Plot)**:
    - Compares the feature distribution to a normal distribution using `stats.probplot()`.
    - Deviations from the diagonal line indicate non-Gaussian behavior.

---

### **13. Applying Box-Cox Transformation**

```python
pt = PowerTransformer(method='box-cox')
X_train_transformed = pt.fit_transform(X_train + 0.000001)
X_test_transformed = pt.transform(X_test + 0.000001)

pd.DataFrame({'cols': X_train.columns, 'box_cox_lambdas': pt.lambdas_})
```
![[Pasted image 20250128163207.png]]
**Explanation:**

- Applied the **Box-Cox transformation** to make the data more Gaussian-like.
- Added a small constant (`0.000001`) to handle zero or negative values.
- Displayed the optimal lambda values for each feature.

---

### **14. Evaluating Model Performance After Box-Cox Transformation**

```python
lr = LinearRegression()
lr.fit(X_train_transformed, y_train)
y_pred2 = lr.predict(X_test_transformed)
r2_score(y_test, y_pred2)
```

**Output:** `0.8047825006181187`

**Explanation:**

- Trained the model on the transformed data.
- The **R2 score improved significantly** to **80.48%**, indicating better model performance.

---

### **15. Cross-Validation After Box-Cox Transformation**

```python
pt = PowerTransformer(method='box-cox')
X_transformed = pt.fit_transform(X + 0.0000001)

lr = LinearRegression()
np.mean(cross_val_score(lr, X_transformed, y, scoring='r2'))
```

**Output:** `0.6658537942219861`

**Explanation:**

- Performed cross-validation on the transformed data.
- The cross-validated R2 score improved to **66.59%**, showing more consistent performance.

---

### **16. Visualizing Distributions After Box-Cox Transformation**

```python
X_train_transformed = pd.DataFrame(X_train_transformed, columns=X_train.columns)

for col in X_train_transformed.columns:
    plt.figure(figsize=(14, 4))
    plt.subplot(121)
    sns.distplot(X_train[col])
    plt.title(col)

    plt.subplot(122)
    sns.distplot(X_train_transformed[col])
    plt.title(col)

    plt.show()
```

![[Pasted image 20250128163343.png]]
![[Pasted image 20250128163359.png]]
![[Pasted image 20250128163412.png]]

**Explanation:**

- **Left Plot (Original Data)**:
    - Shows the original distribution of each feature.
- **Right Plot (Transformed Data)**:
    - Shows the distribution after applying the Box-Cox transformation.
    - The transformed data is closer to a Gaussian distribution.

---

### **17. Applying Yeo-Johnson Transformation**

```python
pt = PowerTransformer(method='yeo-johnson')
X_transformed2 = pt.fit_transform(X)

lr = LinearRegression()
np.mean(cross_val_score(lr, X_transformed2, y, scoring='r2'))
```

**Output:** `0.6834625134285746`

**Explanation:**

- Applied the **Yeo-Johnson transformation**, which is more flexible than Box-Cox as it handles zero and negative values.
- The cross-validated R2 score improved to **68.35%**.

---

### **18. Visualizing Distributions After Yeo-Johnson Transformation**

```python
X_train_transformed2 = pd.DataFrame(X_train_transformed2, columns=X_train.columns)

for col in X_train_transformed2.columns:
    plt.figure(figsize=(14, 4))
    plt.subplot(121)
    sns.distplot(X_train[col])
    plt.title(col)

    plt.subplot(122)
    sns.distplot(X_train_transformed2[col])
    plt.title(col)

    plt.show()
```

![[Pasted image 20250128164622.png]]
![[Pasted image 20250128164644.png]]
![[Pasted image 20250128164706.png]]

**Explanation:**

- **Left Plot (Original Data)**:
    - Shows the original distribution of each feature.
- **Right Plot (Transformed Data)**:
    - Shows the distribution after applying the Yeo-Johnson transformation.
    - The transformed data is closer to a Gaussian distribution.

---

### **19. Comparing Box-Cox and Yeo-Johnson Lambdas**

```python
pd.DataFrame({'cols': X_train.columns, 'box_cox_lambdas': pt.lambdas_, 'yeo_johnson_lambdas': pt1.lambdas_})
```
![[Pasted image 20250128164734.png]]
**Explanation:**

- Displayed the optimal lambda values for both **Box-Cox** and **Yeo-Johnson** transformations.
- This comparison helps understand how each transformation affects the data.

---

### **Summary of Results**

| Transformation Method | R2 Score (Test Set) | Cross-Validated R2 Score |
|-----------------------|---------------------|--------------------------|
| **No Transformation** | 62.75%              | 46.10%                   |
| **Box-Cox**           | 80.48%              | 66.59%                   |
| **Yeo-Johnson**       | -                   | 68.35%                   |

**Key Takeaways:**

1. **Transformations Improve Performance**:
    - Both Box-Cox and Yeo-Johnson transformations significantly improved the model's R2 score and cross-validated performance.
2. **Yeo-Johnson is More Flexible**:
    - Yeo-Johnson handles zero and negative values, making it more versatile than Box-Cox.
3. **Visualizations Confirm Effectiveness**:
    - The distribution plots clearly show that the transformed data is closer to a Gaussian distribution, which is ideal for linear regression.

By applying these transformations, the model's predictive power and consistency were enhanced, demonstrating the importance of preprocessing in machine learning workflows.