**Status:**  #Complete 
**Tags:**  [[Data Preprocessing]]  [[Feature Engineering]] [[Binning]]
### **1. Importing Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.compose import ColumnTransformer
```

**Explanation:**
- **Pandas** and **Numpy**: For data manipulation and numerical operations.
- **Matplotlib**: For visualizing data distributions.
- **Scikit-learn**:
  - `train_test_split`: Splits data into training and testing sets.
  - `DecisionTreeClassifier`: Implements a decision tree model.
  - `accuracy_score`: Evaluates model accuracy.
  - `cross_val_score`: Performs cross-validation.
  - `KBinsDiscretizer`: Discretizes continuous features into bins.
  - `ColumnTransformer`: Applies transformations to specific columns.

---

### **2. Loading and Preparing Data**

```python
df = pd.read_csv('train.csv', usecols=['Age', 'Fare', 'Survived'])
df.dropna(inplace=True)
df.shape
#(714, 3)
df.head()
```
![[Pasted image 20250128201004.png]]

**Explanation:**
- Loaded the Titanic dataset (`train.csv`) with columns: `Age`, `Fare`, and `Survived`.
- Dropped rows with missing values using `dropna()`.
- Checked the shape and first few rows of the dataset.

---

### **3. Splitting Data into Features and Target**

```python
X = df.iloc[:, 1:]  # Features: Age, Fare
y = df.iloc[:, 0]   # Target: Survived
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.head(2)
```
![[Pasted image 20250128201018.png]]

**Explanation:**
- `X` contains the features (`Age` and `Fare`).
- `y` contains the target variable (`Survived`).
- Split the data into training (80%) and testing (20%) sets.

---

### **4. Training a Decision Tree Classifier (Without Discretization)**

```python
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)
```

**Output:** `0.6293706293706294`

**Explanation:**
- Trained a decision tree classifier on the original (non-discretized) data.
- Evaluated the model's accuracy on the test set.

---

### **5. Cross-Validation (Without Discretization)**

```python
np.mean(cross_val_score(DecisionTreeClassifier(), X, y, cv=10, scoring='accuracy'))
```

**Output:** `0.6289123630672926`

**Explanation:**
- Performed 10-fold cross-validation to evaluate the model's consistency.

---

### **6. Applying KBinsDiscretizer**

```python
kbin_age = KBinsDiscretizer(n_bins=15, encode='ordinal', strategy='quantile')
kbin_fare = KBinsDiscretizer(n_bins=15, encode='ordinal', strategy='quantile')

trf = ColumnTransformer([
    ('first', kbin_age, [0]),  # Apply to Age
    ('second', kbin_fare, [1]) # Apply to Fare
])

X_train_trf = trf.fit_transform(X_train)
X_test_trf = trf.transform(X_test)
```

**Explanation:**
- Created two `KBinsDiscretizer` objects:
  - `n_bins=15`: Number of bins.
  - `encode='ordinal'`: Encodes bins as integers.
  - `strategy='quantile'`: Bins are created based on quantiles (equal frequency).
- Applied the transformations to `Age` and `Fare` using `ColumnTransformer`.

---

### **7. Inspecting Bin Edges**

```python
trf.named_transformers_['first'].bin_edges_
```

**Output:**
```
array([array([ 0.42,  6.  , 16.  , 19.  , 21.  , 23.  , 25.  , 28.  , 30.  ,
       32.  , 35.  , 38.  , 42.  , 47.  , 54.  , 80.  ])], dtype=object)
```

**Explanation:**
- Displays the bin edges for the `Age` feature.

---

### **8. Creating a DataFrame for Comparison**

```python
output = pd.DataFrame({
    'age': X_train['Age'],
    'age_trf': X_train_trf[:, 0],
    'fare': X_train['Fare'],
    'fare_trf': X_train_trf[:, 1]
})

output['age_labels'] = pd.cut(x=X_train['Age'], bins=trf.named_transformers_['first'].bin_edges_[0].tolist())
output['fare_labels'] = pd.cut(x=X_train['Fare'], bins=trf.named_transformers_['second'].bin_edges_[0].tolist())

output.sample(5)
```
![[Pasted image 20250128201105.png]]
**Explanation:**
- Created a DataFrame to compare the original and transformed values.
- Added bin labels for `Age` and `Fare`.

---

### **9. Training a Decision Tree Classifier (With Discretization)**

```python
clf = DecisionTreeClassifier()
clf.fit(X_train_trf, y_train)
y_pred2 = clf.predict(X_test_trf)
accuracy_score(y_test, y_pred2)
```

**Output:** `0.6363636363636364`

**Explanation:**
- Trained a decision tree classifier on the discretized data.
- Evaluated the model's accuracy on the test set.

---

### **10. Cross-Validation (With Discretization)**

```python
X_trf = trf.fit_transform(X)
np.mean(cross_val_score(DecisionTreeClassifier(), X_trf, y, cv=10, scoring='accuracy'))
```

**Output:** `0.6275625978090766`

**Explanation:**
- Performed 10-fold cross-validation on the discretized data.

---

### **11. Visualizing Discretization Effects**

```python
def discretize(bins, strategy):
    kbin_age = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy=strategy)
    kbin_fare = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy=strategy)
    
    trf = ColumnTransformer([
        ('first', kbin_age, [0]),
        ('second', kbin_fare, [1])
    ])
    
    X_trf = trf.fit_transform(X)
    print(np.mean(cross_val_score(DecisionTreeClassifier(), X_trf, y, cv=10, scoring='accuracy')))
    
    plt.figure(figsize=(14, 4))
    plt.subplot(121)
    plt.hist(X['Age'])
    plt.title("Before")

    plt.subplot(122)
    plt.hist(X_trf[:, 0], color='red')
    plt.title("After")

    plt.show()
    
    plt.figure(figsize=(14, 4))
    plt.subplot(121)
    plt.hist(X['Fare'])
    plt.title("Before")

    plt.subplot(122)
    plt.hist(X_trf[:, 1], color='red')
    plt.title("Fare")

    plt.show()

discretize(5, 'kmeans')
```

**Output:** `0.6303208137715179`
![[Pasted image 20250128201120.png]]

**Explanation:**
- Defined a function `discretize` to apply discretization and visualize the effects.
- Compared the distribution of `Age` and `Fare` before and after discretization.
- Used `kmeans` strategy for binning.

---

### **Key Takeaways**

1. **Discretization**:
   - Converts continuous features into discrete bins.
   - Can improve model performance by handling outliers and reducing noise.
2. **KBinsDiscretizer**:
   - Supports strategies like `quantile`, `uniform`, and `kmeans`.
   - Encodes bins as ordinal or one-hot values.
3. **Impact on Model**:
   - Discretization slightly improved the accuracy of the decision tree classifier.
   - Cross-validation scores remained consistent.

