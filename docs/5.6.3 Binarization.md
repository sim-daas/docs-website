**Status:**  #Complete 
**Tags:**    [[Data Preprocessing]]  [[Feature Engineering]] 

In **feature engineering**, **binarization** is a technique used to transform continuous or categorical features into binary values (0 or 1). This is particularly useful when you want to simplify data or create binary indicators for specific conditions. Binarization is commonly used in machine learning to preprocess features before feeding them into models.

---

### **How Binarization Works in Feature Engineering**
1. **Thresholding**:
   - For continuous features, a threshold is defined. Values above the threshold are set to 1, and values below or equal to the threshold are set to 0.
   - Example: If you have a feature like "age" and set a threshold of 30, ages above 30 become 1, and ages 30 or below become 0.

2. **Categorical Features**:
   - For categorical features, binarization is often achieved through **one-hot encoding**, where each category is converted into a binary feature (0 or 1).
   - Example: A feature like "color" with categories ["red", "green", "blue"] can be binarized into three binary features: "is_red", "is_green", and "is_blue".

---

### **When to Use Binarization**
- When you want to create binary indicators for specific conditions (e.g., "is_high_income").
- When working with algorithms that perform better with binary inputs (e.g., decision trees, logistic regression).
- When simplifying continuous features into binary categories is meaningful for the problem.

### **Advantages of Binarization**
- Simplifies complex features into binary representations.
- Reduces the impact of outliers in continuous features.
- Can improve interpretability of features.

---

### **Disadvantages of Binarization**
- Loss of information, especially for continuous features.
- May not always capture the relationship between the feature and the target variable effectively.
- Can lead to an increase in the number of features (e.g., in one-hot encoding), which may affect model performance.

---
## **1. Importing Libraries**

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import Binarizer
```

### **Purpose of Libraries**

- **`numpy` and `pandas`**: Used for numerical computations and data manipulation.
- **`train_test_split` and `cross_val_score`**: For splitting data into training/testing sets and performing cross-validation.
- **`DecisionTreeClassifier`**: A classification model based on decision trees.
- **`accuracy_score`**: To evaluate the accuracy of predictions.
- **`ColumnTransformer` and `Binarizer`**: For feature transformations (e.g., binarizing data).

---

## **2. Loading and Preparing Data**

### **Code**

```python
df = pd.read_csv('train.csv')[['Age', 'Fare', 'SibSp', 'Parch', 'Survived']]
df.dropna(inplace=True)
df.head()
```
![[Pasted image 20250128202515.png]]

### **Explanation**

- **Loading Data**: Reads the Titanic dataset (`train.csv`) and selects relevant columns:
    - `Age`, `Fare`, `SibSp` (siblings/spouses aboard), `Parch` (parents/children aboard), and `Survived` (target variable).
- **Handling Missing Values**: Drops rows with missing data to ensure clean input.
- **Preview Data**: `df.head()` displays the first 5 rows for verification.

---

## **3. Feature Engineering**

### **Code**

```python
df['family'] = df['SibSp'] + df['Parch']  # Create a 'family' feature
```
![[Pasted image 20250128202525.png]]
```python
df.drop(columns=['SibSp', 'Parch'], inplace=True)  # Remove unnecessary columns
df.head()  # Verify changes
```
![[Pasted image 20250128202632.png]]
### **Explanation**

- **New Feature**: Combines `SibSp` and `Parch` into a single feature, `family`, to represent the total number of family members.
- **Clean Up**: Removes the original columns to avoid redundancy.

---

## **4. Splitting Data**

### **Code**

```python
X = df.drop(columns=['Survived'])  # Features
y = df['Survived']  # Target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### **Explanation**

- **Feature Matrix (`X`)**: Contains input variables (`Age`, `Fare`, `family`).
- **Target Variable (`y`)**: Indicates survival (`0` = not survived, `1` = survived).
- **Train-Test Split**: Splits the data into 80% training and 20% testing sets for model evaluation.

---

## **5. Model Training and Evaluation (Without Binarization)**

### **Code**

```python
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)  # Train the model
y_pred = clf.predict(X_test)  # Predict on test data
 accuracy_score(y_test, y_pred)
 ``` 
 **Output:** 0.6293706293706294
```python
cv_score = cross_val_score(DecisionTreeClassifier(), X, y, cv=10, scoring='accuracy')
print(np.mean(cv_score))
```
**Output:** 0.6429381846635367
### **Explanation**

- **Training**: Trains a Decision Tree classifier on the training data.
- **Prediction**: Generates predictions for the test set.
- **Evaluation**: Measures accuracy on the test set (e.g., ~62.9%).
- **Cross-Validation**: Computes 10-fold cross-validation accuracy (e.g., ~64.3%).

---

## **6. Applying Binarization**

### **Code**

```python
trf = ColumnTransformer([
    ('bin', Binarizer(copy=False), ['family'])
], remainder='passthrough')

X_train_trf = trf.fit_transform(X_train)  # Transform training data
X_test_trf = trf.transform(X_test)  # Transform test data
```

### **Explanation**

- **Purpose**: Converts the `family` feature into binary values (`0` or `1`) using a threshold (default is `0`).
- **ColumnTransformer**: Applies `Binarizer` only to the `family` column, leaving other columns (`Age`, `Fare`) unchanged.

---

## **7. Model Training and Evaluation (With Binarization)**

### **Code**

```python
clf = DecisionTreeClassifier()
clf.fit(X_train_trf, y_train)  # Train the model on binarized data
y_pred2 = clf.predict(X_test_trf)  # Predict on test data
print("Accuracy (With Binarization):", accuracy_score(y_test, y_pred2))  # Evaluate accuracy
```
**Output:** 0.6363636363636364
```python
X_trf = trf.fit_transform(X)  # Transform entire dataset
cv_score_bin = cross_val_score(DecisionTreeClassifier(), X_trf, y, cv=10, scoring='accuracy')
print("Cross-Validation Accuracy (With Binarization):", np.mean(cv_score_bin))
```
**Output:**  0.6304186228482003
### **Explanation**

- **Training**: Trains the model using the binarized training data.
- **Prediction**: Evaluates the model on the binarized test set (e.g., ~63.6% accuracy).
- **Cross-Validation**: Measures performance using 10-fold cross-validation on the binarized dataset (e.g., ~63.0%).

---

### **Key Insights**

- **Without Binarization**: Slightly higher cross-validation accuracy (~64.3%).
- **With Binarization**: Provides a more interpretable feature (`family`) with a small tradeoff in accuracy (~63.0%).

