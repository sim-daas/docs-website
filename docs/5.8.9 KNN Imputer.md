**Status:**  #Complete 
**Tags:**  [[Data Preprocessing]]  [[Feature Engineering]] [[Missing Value]]
### Notes on Titanic Dataset Classification with Imputation Techniques

This notebook demonstrates how to handle missing values in the Titanic dataset using two different imputation techniques: **KNN Imputer** and **Simple Imputer (Mean)**. We then train a Logistic Regression model to predict survival and compare the accuracy of the two imputation methods.

---

### **1. Importing Libraries**
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
```
- **numpy**: For numerical computations.
- **pandas**: For data manipulation and analysis.
- **train_test_split**: To split the dataset into training and testing sets.
- **KNNImputer**: For imputing missing values using the K-Nearest Neighbors algorithm.
- **SimpleImputer**: For imputing missing values using a simple strategy (e.g., mean, median).
- **LogisticRegression**: A classification model used to predict survival.
- **accuracy_score**: To evaluate the model's performance.

---

### **2. Loading and Preparing the Data**
```python
df = pd.read_csv('train.csv')[['Age', 'Pclass', 'Fare', 'Survived']]
df.head()
```
- We load the Titanic dataset (`train.csv`) and select only the relevant columns: `Age`, `Pclass`, `Fare`, and `Survived`.
- `df.head()` displays the first few rows of the dataset.

---

### **3. Checking for Missing Values**
```python
df.isnull().mean() * 100
```
- This line calculates the percentage of missing values in each column.
- For example, if `Age` has 20% missing values, it means 20% of the rows in the `Age` column are `NaN`.

---

### **4. Splitting the Data into Features and Target**
```python
X = df.drop(columns=['Survived'])
y = df['Survived']
```
- `X`: Features (independent variables) - `Age`, `Pclass`, and `Fare`.
- `y`: Target (dependent variable) - `Survived` (1 for survived, 0 for not survived).

---

### **5. Splitting the Data into Training and Testing Sets**
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
```
- The dataset is split into:
  - **Training set**: 80% of the data (`X_train`, `y_train`).
  - **Testing set**: 20% of the data (`X_test`, `y_test`).
- `random_state=2` ensures reproducibility.

---

### **6. Handling Missing Values with KNN Imputer**
```python
knn = KNNImputer(n_neighbors=3, weights='distance')
X_train_trf = knn.fit_transform(X_train)
X_test_trf = knn.transform(X_test)
```
- **KNNImputer**: Imputes missing values by finding the `n_neighbors` nearest neighbors (here, 3) and using their values to fill in the missing data.
- `weights='distance'`: Closer neighbors have more influence on the imputed value.
- `fit_transform`: Fits the imputer on the training data and transforms it.
- `transform`: Applies the same imputation to the test data.

---

### **7. Training a Logistic Regression Model with KNN Imputed Data**
```python
lr = LogisticRegression()
lr.fit(X_train_trf, y_train)
y_pred = lr.predict(X_test_trf)
accuracy_score(y_test, y_pred)
```
- **LogisticRegression**: A classification model used to predict survival.
- The model is trained on the KNN-imputed training data (`X_train_trf`).
- Predictions are made on the KNN-imputed test data (`X_test_trf`).
- **Accuracy**: 71.51% (this may vary slightly due to randomness).

---

### **8. Handling Missing Values with Simple Imputer (Mean)**
```python
si = SimpleImputer()
X_train_trf2 = si.fit_transform(X_train)
X_test_trf2 = si.transform(X_test)
```
- **SimpleImputer**: Imputes missing values using a simple strategy (default is `mean`).
- Missing values in each column are replaced with the mean of that column.
- `fit_transform` and `transform` are used similarly to the KNN Imputer.

---

### **9. Training a Logistic Regression Model with Mean Imputed Data**
```python
lr = LogisticRegression()
lr.fit(X_train_trf2, y_train)
y_pred2 = lr.predict(X_test_trf2)
accuracy_score(y_test, y_pred2)
```
- The same Logistic Regression model is trained on the mean-imputed data.
- **Accuracy**: 69.27% (slightly lower than KNN Imputer).

---

### **10. Comparison of Results**
- **KNN Imputer Accuracy**: 71.51%
- **Simple Imputer (Mean) Accuracy**: 69.27%

**Conclusion**:
- KNN Imputer performs slightly better than Simple Imputer (Mean) for this dataset.
- KNN Imputer considers the relationships between features, which may lead to more accurate imputation.
- Simple Imputer is faster but may not capture complex patterns in the data.

---

### **Key Takeaways**
1. **Imputation Techniques**:
   - **KNN Imputer**: More accurate but computationally expensive.
   - **Simple Imputer**: Faster but less accurate for complex datasets.
2. **Model Performance**:
   - The choice of imputation method can impact the accuracy of the model.
3. **Next Steps**:
   - Experiment with other imputation techniques (e.g., median, mode).
   - Try different classification models (e.g., Random Forest, Gradient Boosting).
