**Status:**  #Complete 
**Tags:**  [[EDA]]  [[Data Preprocessing]]  [[Feature Engineering]] [[Outliers]]
### **What Are Outliers?**

Outliers are data points that significantly deviate from the rest of the dataset. They can be **much higher or lower** than most other observations and may arise due to measurement errors, variability in the data, or anomalies.

---

### **When Are Outliers Dangerous?**

Outliers can be dangerous in the following cases:

1. **They distort statistical measures**:
    
    - **Mean** is highly affected, making it unrepresentative of the dataset.
    - **Standard deviation** increases, leading to misleading variability.
2. **They negatively impact machine learning models**:
    
    - **Regression models (e.g., Linear Regression)**: Outliers can pull the regression line, leading to poor predictions.
    - **Clustering algorithms (e.g., K-Means)**: Centroids shift due to outliers, leading to incorrect cluster formation.
    - **Weight-based algorithms (e.g., Neural Networks, Logistic Regression)**: Outliers can cause large gradient updates, leading to unstable training.
3. **They lead to incorrect conclusions**:
    
    - In hypothesis testing, outliers can lead to false positives or negatives.

---

### **Algorithms Affected by Outliers**

Some algorithms are highly **sensitive** to outliers, while others are **robust**.

#### **1️⃣ Affected (Sensitive to Outliers)**

- **Linear Regression** (outliers shift the regression line)
- **Logistic Regression** (weight updates are influenced)
- **K-Means Clustering** (centroids get misplaced)
- **Principal Component Analysis (PCA)** (variance is influenced)
- **Neural Networks** (gradients can explode due to large values)

#### **2️⃣ Robust (Less Affected)**

- **Decision Trees & Random Forest** (split rules limit the impact)
- **Gradient Boosting (XGBoost, LightGBM)** (outliers have less impact)
- **K-Nearest Neighbors (KNN)** (distance-based, but can still be affected)
- **Support Vector Machines (SVM)** (if using an appropriate kernel)

---

### **How to Treat Outliers?**

1. **Trimming (Removing Outliers)**
    
    - Remove extreme values based on **percentile cutoff** (e.g., 1st and 99th percentile).
    - Used when **outliers are due to errors** and do not carry useful information.
2. **Capping (Winsorization)**
    
    - Replace extreme values with a threshold (e.g., cap at 95th percentile).
    - Used when **outliers exist but should not have extreme influence**.
3. **Treating as Missing Values**
    
    - If outliers are likely due to **data entry errors**, replace them with **NaN** and use imputation.
4. **Transforming the Data**
    
    - Use **log transformation, square root, or Box-Cox transformation** to reduce the impact of large values.
5. **Using Robust Models**
    
    - Algorithms like **Tree-based models** (Random Forest, XGBoost) are **less affected** by outliers.

---

### **How to Detect Outliers?**

Detection methods depend on the distribution of data:

#### **1️⃣ Normal Distribution (Gaussian)**

- **Z-Score Method**(Normal Distribution):
$$Z= \frac{x - \mu}{\sigma}$$

##### Where:

- x = Individual data point
- μ= Mean 
- σ = Standard deviation

##### How It Works:

- The Z-score tells us how many **standard deviations** a data point is from the mean.
- If ∣Z∣>3|Z| > 3, the point is usually considered an **outlier** in normally distributed data.


- ## **Box Plot (IQR Method)**:
#### **What is IQR?**

The **Interquartile Range (IQR)** is a measure of statistical dispersion, which describes the middle **50% of a dataset**. It is calculated as:

$$IQR=Q3−Q1$$

Where:

- **Q1 (First Quartile)** = 25th percentile (Lower bound of middle 50%)
- **Q3 (Third Quartile)** = 75th percentile (Upper bound of middle 50%)

---

### **How to Identify Outliers?**

A data point xx is considered an **outlier** if it falls outside the following range:

$$x<Q1−1.5×IQR \quad \text{or} \quad x>Q3+1.5×IQR$$

- **Lower Bound**: $$Q1 - 1.5 \times IQR$$
- **Upper Bound**: $$Q3 + 1.5 \times IQR$$

Any data points **below the lower bound** or **above the upper bound** are considered outliers.

---

### **Advantages of Using IQR for Outlier Detection**

✔ **Robust to Skewed Data**: Unlike Z-score, IQR does not assume normality.  
✔ **Simple and Effective**: Easy to compute and understand.  
✔ **Less Sensitive to Extreme Values**: Unlike mean and standard deviation, IQR is based on percentiles.
### **Summary**

|Term|Definition|
|---|---|
|**Q1 (25th Percentile)**|The value below which 25% of the data falls|
|**Q3 (75th Percentile)**|The value below which 75% of the data falls|
|**IQR**|The range covering the middle 50% of data (Q3 - Q1)|
|**Outlier Detection Rule**|Any value outside **1.5 × IQR** is an outlier|

---

#### **2️⃣ Skewed Distribution**

- **Modified Z-Score**:
    - Uses **Median Absolute Deviation (MAD)** instead of standard deviation.
    - More **robust** for skewed data.

#### **3️⃣ Other Distributions**

- **Isolation Forest (Machine Learning)**
    - Works well for both normal and non-normal data.
- **DBSCAN (Clustering Algorithm)**
    - Detects density-based outliers.

---

### **Summary**

|Aspect|Methods|
|---|---|
|**Detecting Outliers**|Z-Score, IQR, MAD, Isolation Forest, DBSCAN|
|**Treating Outliers**|Trimming, Capping, Missing Value Treatment, Transformation|
|**Algorithms Affected**|Linear Regression, Logistic Regression, K-Means, PCA, Neural Networks|
|**Robust Algorithms**|Decision Trees, Random Forest, SVM (with proper kernel), Gradient Boosting|
