**Status:**  #Complete 
**Tags:** [[Algorithm]] [[Regression]]

---
### **Overview**

- **Definition**: Multiple Linear Regression (MLR) is a statistical technique used to predict the value of a dependent variable based on multiple independent (predictor) variables.
- **Key Use Cases**: Widely applied in economics, finance, healthcare, and social sciences to understand relationships between multiple factors and outcomes.
- **Objective**: Quantify the effect of multiple predictors on a response variable and make predictions.

---

### **Key Concepts**

1. **Predictors and Dependent Variable**:
    
    - **Predictor variables (X₁, X₂, ..., Xₖ)**: Independent variables influencing the dependent variable Y.
    - **Dependent variable (Y)**: The outcome variable being predicted.
2. **Model Equation**:
    
    - General form:  
        $$Y=β_0+β_1X_1+β_2X_2+β_3X_3+...+β_kX_k$$$$\beta_0 (Intercept): Constant \ term.$$$$\beta_i (Coefficients): Represents \ the\ weight\ of\ each\ predictor.$$        $$\varepsilon (Error\ term): Captures\ unaccounted\ variability.$$
3. **Estimation Method**:
    
    - **Ordinary Least Squares (OLS)**: Minimizes the sum of squared errors to estimate coefficients.
4. **Inference and Interpretation**:
    
    - **P-values**: Assess significance of predictors.
    - **Confidence Intervals**: Estimate uncertainty in coefficient values.
    - **R-squared**: Measures the proportion of variance in Y explained by predictors.
    - **Adjusted R-squared**: Adjusts for model complexity to prevent overfitting.
5. **Interpretation**:
    
    - **Significant coefficients**: Meaningful relationships between predictors and Y.
    - **Coefficient magnitude**: Strength of each predictor’s influence.

---

### **Assumptions of Multiple Linear Regression**

1. **Linearity**: The relationship between predictors and Y is linear.
2. **Additivity**: The effect of each predictor is additive.
3. **Homoscedasticity**: Constant variance of errors across all observations.
4. **No Multicollinearity**: Predictors should not be highly correlated.
5. **No Autocorrelation**: Errors should be randomly distributed (especially in time-series data).
6. **Normality of Errors**: Residuals should follow a normal distribution.

---
### **Steps to Build a Multiple Linear Regression Model**

1. **Data Preparation**:
    
    - Handle missing values.
    - Remove outliers and normalize/scale features if needed.
    - Check for multicollinearity using Variance Inflation Factor (VIF).
2. **Model Specification**:
    
    - Identify key predictors based on domain knowledge and correlation analysis.
    - Ensure assumptions are met.
3. **Model Training and Evaluation**:
    
    - Fit the model using OLS.
    - Interpret coefficients and evaluate goodness-of-fit.
    - Perform residual diagnostics.
4. **Optimization and Feature Selection**:
    
    - Apply **Forward Selection**, **Backward Elimination**, or **Stepwise Regression**.
    - Use cross-validation to validate model performance.

---

### **Code Implementation in Python**

#### **1. Import Necessary Libraries**

```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

#### **2. Load and Prepare Data**

```python
# Load dataset (example: housing data)
data = pd.read_csv("data.csv")

# Define independent variables (X) and dependent variable (Y)
X = data[["feature1", "feature2", "feature3"]]  # Replace with actual feature names
Y = data["target"]

# Add constant term for intercept
X = sm.add_constant(X)

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
```

#### **3. Fit the Regression Model**

```python
# Fit model using OLS
model = sm.OLS(Y_train, X_train).fit()

# Print summary of the model
print(model.summary())
```

#### **4. Make Predictions and Evaluate Model**

```python
# Predict on test data
Y_pred = model.predict(X_test)

# Evaluate model performance
rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))
r2 = r2_score(Y_test, Y_pred)

print(f"RMSE: {rmse}")
print(f"R-squared: {r2}")
```

---

### **Example Application**

**Predicting House Prices**:

- **Dependent Variable (Y)**: House price.
- **Predictors (X₁, X₂, X₃, ...)**:
    - Location rating
    - Number of bedrooms
    - Square footage
    - Proximity to schools

---

### **Conclusion**

Multiple Linear Regression is a powerful technique for modeling relationships between multiple predictors and a dependent variable. By carefully selecting features, checking assumptions, and optimizing model performance, MLR can provide valuable insights and accurate predictions across various domains.