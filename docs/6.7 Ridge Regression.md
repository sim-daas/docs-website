#### Overview

**Ridge Regression**, also known as **L2 Regularization**, is a technique used to address the issue of multicollinearity and overfitting in linear regression. It modifies the cost function by adding a penalty term proportional to the square of the magnitude of the coefficients.

---

### Key Points

- Ridge Regression **shrinks the coefficients** towards zero but does not set them exactly to zero (unlike Lasso).
- Useful when there is **multicollinearity** (high correlation between features) or when the model is **overfitting**.
- Balances the tradeoff between fitting the training data and keeping the model complexity low.

---

### Cost Function

The Ridge Regression cost function is:

$$J(β)=RSS+λ∑j=1nβj2J(\beta) = \text{RSS} + \lambda \sum_{j=1}^n \beta_j^2$$

Where:

$$J(β)J(\beta):\ Cost\ function\ (to\ minimize).$$
$$ RSS: Residual\ Sum\ of\ Squares.$$
$$\$$lambda:\ Regularization\ parameter\ (controls\ the\ penalty\ term).$$
$$\beta_j: \ Model \ coefficients.$$

#### Key Aspects of the Cost Function:

1. **When λ=0**:
    - Ridge Regression becomes equivalent to ordinary least squares (OLS) regression.
2. **When λ is large**:
    - Coefficients are heavily penalized and shrink closer to zero.
    - Risk of underfitting increases.

---

### Advantages

3. **Prevents Overfitting**:
    - Reduces the impact of highly correlated features.
4. **Stable Estimates**:
    - Handles multicollinearity by shrinking coefficients without eliminating features.
5. **Improves Generalization**:
    - Reduces the model's sensitivity to training data noise.

---

### Disadvantages

6. **Bias Introduction**:
    - Adds bias to the model in exchange for lower variance.
7. **Feature Interpretability**:
    - Coefficients are shrunk, making interpretation harder.
8. **Ineffective for Irrelevant Features**:
    - Does not set coefficients to zero, so all features remain in the model.

---

### Hyperparameter λ (Regularization Parameter)

- **Choosing λ
    - Use **cross-validation** to find the optimal λ\lambda value.
    - High λ results in more regularization (higher bias, lower variance).
    - Low λ results in less regularization (lower bias, higher variance).

---

### Comparison to Lasso Regression

|Feature|Ridge Regression|Lasso Regression|
|---|---|---|
|Regularization Type|L2|L1|
|Coefficient Effect|Shrinks coefficients|Shrinks and sets some to 0|
|Feature Selection|Does not eliminate features|Performs feature selection|
|Use Case|Multicollinearity issues|Sparse feature spaces|

---

### Python Implementation

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Sample Data
X, y = datasets.load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ridge Regression
ridge = Ridge(alpha=1.0)  # Alpha is the regularization parameter (lambda)
ridge.fit(X_train, y_train)

# Predictions and Evaluation
y_pred = ridge.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
```

---

### Applications

9. **Economics**:
    - Predicting economic indicators in the presence of multicollinearity.
10. **Genetics**:
    - Modeling relationships in high-dimensional genomic data.
11. **Finance**:
    - Predicting stock prices using highly correlated features.

---

### Key Takeaways

- Ridge Regression is effective in addressing overfitting and multicollinearity.
- It regularizes the coefficients without completely removing features.
- The choice of λ\lambda significantly affects the model's performance and requires fine-tuning.